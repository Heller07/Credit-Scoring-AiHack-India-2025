{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble CatBoost + XGBoost (Optuna 10-Fold)\n",
        "\n",
        "This notebook reuses the full feature-engineering pipeline from `catboost_finalsolution.ipynb`, performs fresh 10-fold Optuna tuning for both CatBoost and XGBoost, and then blends the two models to produce `ensemble_submission.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q catboost xgboost optuna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Rebuild feature set via `catboost_finalsolution.ipynb`\n",
        "\n",
        "Running the original notebook ensures every engineered feature (150+) is available here. The notebook will also train a baseline CatBoost model; we only reuse the engineered `train`, `test`, `features`, and helper lists for the fresh tuning below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%run ./catboost_finalsolution.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "np.random.seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "required_vars = ['train', 'test', 'features', 'TARGET', 'categorical_features']\n",
        "for var in required_vars:\n",
        "    if var not in globals():\n",
        "        raise ValueError(f\"Missing '{var}' from catboost_finalsolution.ipynb run. Please ensure the notebook executed successfully.\")\n",
        "\n",
        "print(f\"Feature count: {len(features)} | Train shape: {train.shape} | Test shape: {test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET = 'Default 12 Flag'\n",
        "y = train[TARGET]\n",
        "X = train[features]\n",
        "X_test = test[features]\n",
        "skf_10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CatBoost: 10-Fold Optuna Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_cat_params(trial):\n",
        "    return {\n",
        "        'iterations': trial.suggest_int('iterations', 1200, 2200),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
        "        'depth': trial.suggest_int('depth', 6, 10),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2.0, 8.0),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 16, 64),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
        "        'bootstrap_type': 'Bernoulli',\n",
        "        'loss_function': 'Logloss',\n",
        "        'eval_metric': 'AUC',\n",
        "        'random_state': 42,\n",
        "        'early_stopping_rounds': 200,\n",
        "        'task_type': 'GPU' if 'USE_GPU' in globals() and USE_GPU else 'CPU',\n",
        "        'devices': '0' if 'USE_GPU' in globals() and USE_GPU else None,\n",
        "        'verbose': False,\n",
        "        'cat_features': categorical_features,\n",
        "    }\n",
        "\n",
        "\n",
        "def cat_objective(trial):\n",
        "    params = build_cat_params(trial)\n",
        "    fold_scores = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf_10.split(X, y), start=1):\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(\n",
        "            X.iloc[trn_idx], y.iloc[trn_idx],\n",
        "            eval_set=(X.iloc[val_idx], y.iloc[val_idx]),\n",
        "            use_best_model=True\n",
        "        )\n",
        "        preds = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
        "        fold_scores.append(roc_auc_score(y.iloc[val_idx], preds))\n",
        "        if fold_scores[-1] < 0.60:  # early prune very weak configs\n",
        "            raise optuna.TrialPruned()\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "cat_study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "cat_study.optimize(cat_objective, n_trials=8, show_progress_bar=True)\n",
        "\n",
        "best_cat_params = build_cat_params(optuna.trial.FixedTrial(cat_study.best_params))\n",
        "best_cat_params.update({'verbose': 200})\n",
        "print(f\"Best CatBoost AUC: {cat_study.best_value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_models = []\n",
        "cat_oof = np.zeros(len(X))\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf_10.split(X, y), start=1):\n",
        "    model = CatBoostClassifier(**best_cat_params)\n",
        "    model.fit(\n",
        "        X.iloc[trn_idx], y.iloc[trn_idx],\n",
        "        eval_set=(X.iloc[val_idx], y.iloc[val_idx]),\n",
        "        use_best_model=True,\n",
        "        verbose=False\n",
        "    )\n",
        "    preds = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
        "    cat_oof[val_idx] = preds\n",
        "    cat_models.append(model)\n",
        "    print(f\"Fold {fold:02d} CatBoost AUC: {roc_auc_score(y.iloc[val_idx], preds):.6f}\")\n",
        "\n",
        "print(f\"\\nCatBoost 10-fold CV AUC: {roc_auc_score(y, cat_oof):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_test_preds = np.mean([model.predict_proba(X_test)[:, 1] for model in cat_models], axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. XGBoost: 10-Fold Optuna Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_xgb_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_xgb = df.copy()\n",
        "    for col in df_xgb.columns:\n",
        "        if str(df_xgb[col].dtype) == 'category':\n",
        "            df_xgb[col] = df_xgb[col].cat.codes.astype('int32')\n",
        "    return df_xgb.fillna(-999)\n",
        "\n",
        "X_xgb = prepare_xgb_matrix(X)\n",
        "X_test_xgb = prepare_xgb_matrix(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_gpu_hist = 'USE_GPU' in globals() and USE_GPU\n",
        "\n",
        "\n",
        "def build_xgb_params(trial):\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        'eta': trial.suggest_float('eta', 0.01, 0.05, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 10.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.95),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-3, 10, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-3, 10, log=True),\n",
        "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
        "    }\n",
        "    if use_gpu_hist:\n",
        "        params.update({'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'})\n",
        "    else:\n",
        "        params.update({'tree_method': 'hist', 'predictor': 'auto'})\n",
        "    return params\n",
        "\n",
        "\n",
        "def xgb_objective(trial):\n",
        "    params = build_xgb_params(trial)\n",
        "    num_boost_round = trial.suggest_int('num_boost_round', 600, 1600)\n",
        "    fold_scores = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf_10.split(X_xgb, y), start=1):\n",
        "        dtrain = xgb.DMatrix(X_xgb.iloc[trn_idx], label=y.iloc[trn_idx])\n",
        "        dvalid = xgb.DMatrix(X_xgb.iloc[val_idx], label=y.iloc[val_idx])\n",
        "        booster = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=num_boost_round,\n",
        "            evals=[(dvalid, 'valid')],\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        preds = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
        "        score = roc_auc_score(y.iloc[val_idx], preds)\n",
        "        fold_scores.append(score)\n",
        "        if score < 0.60:\n",
        "            raise optuna.TrialPruned()\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "xgb_study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=2024))\n",
        "xgb_study.optimize(xgb_objective, n_trials=10, show_progress_bar=True)\n",
        "\n",
        "best_xgb_params = build_xgb_params(optuna.trial.FixedTrial(xgb_study.best_params))\n",
        "best_xgb_boost = xgb_study.best_params['num_boost_round']\n",
        "print(f\"Best XGBoost AUC: {xgb_study.best_value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_models = []\n",
        "xgb_oof = np.zeros(len(X_xgb))\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf_10.split(X_xgb, y), start=1):\n",
        "    dtrain = xgb.DMatrix(X_xgb.iloc[trn_idx], label=y.iloc[trn_idx])\n",
        "    dvalid = xgb.DMatrix(X_xgb.iloc[val_idx], label=y.iloc[val_idx])\n",
        "    booster = xgb.train(\n",
        "        best_xgb_params,\n",
        "        dtrain,\n",
        "        num_boost_round=best_xgb_boost,\n",
        "        evals=[(dvalid, 'valid')],\n",
        "        early_stopping_rounds=200,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    preds = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
        "    xgb_oof[val_idx] = preds\n",
        "    xgb_models.append(booster)\n",
        "    print(f\"Fold {fold:02d} XGB AUC: {roc_auc_score(y.iloc[val_idx], preds):.6f}\")\n",
        "\n",
        "print(f\"\\nXGBoost 10-fold CV AUC: {roc_auc_score(y, xgb_oof):.6f}\")\n",
        "\n",
        "xgb_test_preds = np.mean([\n",
        "    booster.predict(xgb.DMatrix(X_test_xgb), iteration_range=(0, booster.best_iteration + 1))\n",
        "    for booster in xgb_models\n",
        "], axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Blend + Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_weight = 0.55\n",
        "xgb_weight = 1.0 - cat_weight\n",
        "\n",
        "ensemble_oof = cat_weight * cat_oof + xgb_weight * xgb_oof\n",
        "ensemble_score = roc_auc_score(y, ensemble_oof)\n",
        "print(f\"Blend weights -> Cat: {cat_weight:.2f}, XGB: {xgb_weight:.2f}\")\n",
        "print(f\"Ensemble OOF AUC: {ensemble_score:.6f}\")\n",
        "\n",
        "ensemble_test = cat_weight * cat_test_preds + xgb_weight * xgb_test_preds\n",
        "\n",
        "ensemble_submission = pd.DataFrame({\n",
        "    'ID': test['ID'],\n",
        "    'Default 12 Flag': ensemble_test\n",
        "})\n",
        "\n",
        "output_path = 'ensemble_submission.csv'\n",
        "ensemble_submission.to_csv(output_path, index=False)\n",
        "print(f\"Saved {output_path} with shape {ensemble_submission.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
